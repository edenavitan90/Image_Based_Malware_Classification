from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np

malware_family = {"Adialer.C": 0, "Agent.FYI": 1, "Allaple.A": 2, "Allaple.L": 3, "Alueron.gen!J": 4, "Autorun.K": 5, "C2LOP.gen!g": 6,
                  "C2LOP.P": 7, "Dialplatform.B": 8, "Dontovo.A": 9, "Fakerean": 10, "Instantaccess": 11, "Lolyda.AA1": 12, "Lolyda.AA2": 13,
                  "Lolyda.AA3": 14, "Lolyda.AT": 15, "Malex.gen!J": 16, "Obfuscator.AD": 17, "Rbot!gen": 18, "Skintrim.N": 19, "Swizzor.gen!E": 20,
                  "Swizzor.gen!I": 21, "VB.AT": 22, "Wintrim.BX": 23, "Yuner.A": 24}

class_names = np.array(["Adialer.C", "Agent.FYI", "Allaple.A", "Allaple.L", "Alueron.gen!J", "Autorun.K", "C2LOP.gen!g",
                  "C2LOP.P", "Dialplatform.B", "Dontovo.A", "Fakerean", "Instantaccess", "Lolyda.AA1", "Lolyda.AA2",
                  "Lolyda.AA3", "Lolyda.AT", "Malex.gen!J", "Obfuscator.AD", "Rbot!gen", "Skintrim.N", "Swizzor.gen!E",
                  "Swizzor.gen!I", "VB.AT", "Wintrim.BX", "Yuner.A"])


def plot_confusion_matrix(cm, title, labels):
    cmp = metrics.ConfusionMatrixDisplay(cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(17, 17))
    cmp.plot(ax=ax, values_format='.0f')
    plt.title(title)
    plt.xticks(rotation=90)


def plot_hist(class_names, train_count, test_count, cm_train, cm_test):
    train_accuracy_by_family = np.diag(cm_train) / train_count
    test_accuracy_by_family = np.diag(cm_test) / test_count

    fig, ax = plt.subplots(figsize=(16, 10))
    labels = np.arange(len(class_names))
    width = 0.25
    ax.bar(labels - width / 2 - 0.02, test_accuracy_by_family, color="#a8c9a7", width=width, align='center')
    ax.bar(labels + width / 2 + 0.02, train_accuracy_by_family, color="#b55e5f", width=width, align='center')
    fig.tight_layout()
    ax.set_xticks(labels)
    plt.xlim([-1, len(class_names)])
    ax.set_xticklabels(class_names)
    plt.xticks(rotation=90)
    plt.legend(["Test", "Train"], prop={'size': 16})
    plt.ylim([0, 1.11])
    plt.grid(True)
    plt.show()


def run_StratifiedKFold(X, Y, K, model):
    skf = StratifiedKFold(n_splits=K, random_state=1, shuffle=True)

    X = np.array(X)
    Y = np.array(Y)
    train_success = []
    test_success = []
    train_false_classification_total = []
    test_false_classification_total = []
    cm_train_vector = np.zeros((class_names.shape[0], class_names.shape[0]))
    cm_test_vector = np.zeros((class_names.shape[0], class_names.shape[0]))

    y_train_count_mean_array = np.zeros(len(class_names))
    y_test_count_mean_array = np.zeros(len(class_names))

    # clf = LinearDiscriminantAnalysis()

    it = 0
    for train_index, test_index in skf.split(X, Y):
        it += 1
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = Y[train_index], Y[test_index]

        _, train_count = np.unique(y_train, return_counts=True)
        _, test_count = np.unique(y_test, return_counts=True)

        y_train_count_mean_array += np.array(train_count)
        y_test_count_mean_array += np.array(test_count)

        model.fit(
            X_train,
            y_train
        )

        # Train
        y_hat_train = model.predict(X_train)
        cm_train = metrics.confusion_matrix(y_train, y_hat_train)
        cm_train_vector += cm_train

        # Test
        y_hat_test = model.predict(X_test)
        cm_test = metrics.confusion_matrix(y_test, y_hat_test)
        cm_test_vector += cm_test

        train_success_rate = (y_hat_train == y_train).sum() / len(y_train)
        test_success_rate = (y_hat_test == y_test).sum() / len(y_test)
        train_success.append(train_success_rate)
        test_success.append(test_success_rate)

        train_false_classification = class_names[y_train[np.where((y_hat_train == y_train) == False)]]
        train_false_classification_total += train_false_classification.tolist()
        if len(train_false_classification.tolist()) > 0:
            train_false_classification_labels, train_false_classification_count = np.unique(train_false_classification, return_counts=True)
            train_false_classification_to_print = \
                f"{train_false_classification_labels[np.argmax(train_false_classification_count)]} (# of appearance: {train_false_classification_count[np.argmax(train_false_classification_count)]} out of {len(train_false_classification)})"
        else:
            train_false_classification_to_print = f"N/A"

        test_false_classification = class_names[y_test[np.where((y_hat_test == y_test) == False)]]
        test_false_classification_total += test_false_classification.tolist()
        if len(test_false_classification.tolist()) > 0:
            test_false_classification_labels, test_false_classification_count = np.unique(test_false_classification, return_counts=True)
            test_false_classification_to_print = \
                f"{test_false_classification_labels[np.argmax(test_false_classification_count)]} (# of appearance: {test_false_classification_count[np.argmax(test_false_classification_count)]} out of {len(test_false_classification)})"
        else:
            test_false_classification_to_print = f"N/A"

        print(f"\n{K}.{it}) Success Rate (without normalizing - per iteration):\n"
              f"\t- Train: {(train_success_rate * 100):.3f} %\n"
              f"\t- Test: {(test_success_rate* 100):.3f} %\n"
              f"     Malware family with the highest false classification (without normalizing - per iteration):\n"
              f"\t- Train: {train_false_classification_to_print}\n"
              f"\t- Test: {test_false_classification_to_print}\n"
              f"\n----------")

    # Need to take these vals and convert to Dict (each of them) -> family name = key, the count = val
    # Continue with calc as we wrote in the OneNote.
    # can we do it with np.unique?
    train_false_classification_lables, train_false_classification_count = np.unique(
        np.array(train_false_classification_total), return_counts=True)
    test_false_classification_lables, test_false_classification_count = np.unique(
        np.array(test_false_classification_total), return_counts=True)

    train_false_classification_dict = dict(zip(train_false_classification_lables, train_false_classification_count))
    test_false_classification_dict = dict(zip(test_false_classification_lables, test_false_classification_count))

    '''
    _, images_by_class_names_count = np.unique(np.array(Y), return_counts=True)
    images_names_count_dict = dict(zip(class_names, images_by_class_names_count))

    # Train:
    for key in train_false_classification_dict:
        train_false_classification_dict[key] = (train_false_classification_dict[key] * (1-1/K) * 1/K) * 1/images_names_count_dict[key]

    # Test: 
    for key in test_false_classification_dict:
        test_false_classification_dict[key] = (test_false_classification_dict[key] * 1/K * 1/K) * 1/images_names_count_dict[key]
    '''
    train_mean = np.mean(train_success)
    test_mean = np.mean(test_success)

    train_variance = np.var(train_success)
    test_variance = np.var(test_success)

    return (train_mean, test_mean, train_variance, test_variance,
            cm_train_vector / K, cm_test_vector / K, train_false_classification_dict,
            test_false_classification_dict,
            y_train_count_mean_array / K, y_test_count_mean_array / K)


def plot_hist_and_conf(cm_train_vector, cm_test_vector, train_mean_list, test_mean_list, overall_mean_success_rate,
                       train_false_classification_list, test_false_classification_list, y_train_count_list, y_test_count_list):

    for i in range(len(cm_train_vector)):
        print(f"\n\tK = {i + 2}")

        print("\nSuccess Rate (after normalizing):")
        print(f"\t- Train: {(train_mean_list[i] * 100):.3f} %")
        print(f"\t- Test: {(test_mean_list[i] * 100):.3f} %")
        print(f"\t- AVG: {(overall_mean_success_rate[i] * 100):.3f} %")

        if len(train_false_classification_list[i]) > 0:
            train_false_classification_keys = list(train_false_classification_list[i])
            train_false_classification_argmax = np.argmax(np.array(list(train_false_classification_list[i].values())))
            train_false_classification_to_print = f"{train_false_classification_keys[train_false_classification_argmax]}"
        else:
            train_false_classification_to_print = f"N/A"

        if len(test_false_classification_list[i]) > 0:
            test_false_classification_keys = list(test_false_classification_list[i])
            test_false_classification_argmax = np.argmax(np.array(list(test_false_classification_list[i].values())))
            test_false_classification_to_print = f"{test_false_classification_keys[test_false_classification_argmax]}"
        else:
            test_false_classification_to_print = f"N/A"

        print("\nMalware family with the highest false classification (after normalizing):")
        print("\t- Train:", train_false_classification_to_print)
        print("\t- Test:", test_false_classification_to_print)

        print("\n----------")

        plot_confusion_matrix(
            cm_train_vector[i],
            'Train data confusion matrix (K={})\nSUCCESS RATE: {:.2f} %'.format(i + 2, train_mean_list[i] * 100),
            class_names
        )

        plot_confusion_matrix(
            cm_test_vector[i],
            'Test data confusion matrix (K={})\nSUCCESS RATE: {:.2f} %'.format(i + 2, test_mean_list[i] * 100),
            class_names
        )

        plot_hist(class_names, y_train_count_list[i], y_test_count_list[i], cm_train_vector[i], cm_test_vector[i])

        print("\n******************************************")